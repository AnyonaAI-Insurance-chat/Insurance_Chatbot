# chatbot_agente_polizas_ollama.py - Versi√≥n 100% gratuita

import os
from dotenv import load_dotenv
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import Ollama
from langchain_community.tools import DuckDuckGoSearchRun
from langchain_core.tools import Tool

load_dotenv()

# Cargar √≠ndice vectorial con embeddings gratuitos
def cargar_vectordb():
    path = "vector_index/"
    if not os.path.exists(path):
        raise FileNotFoundError("No se encontr√≥ el √≠ndice FAISS. Ejecuta primero indexador_ollama.py.")
    
    # Usar los mismos embeddings que en la indexaci√≥n
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        model_kwargs={'device': 'cpu'}
    )
    
    return FAISS.load_local(path, embeddings, allow_dangerous_deserialization=True)

# Herramienta de consulta local
def crear_tool_pdf(vectordb):
    def consulta(query: str) -> str:
        docs = vectordb.similarity_search(query, k=3)
        print("\n[DEBUG] Chunks recuperados por similarity_search:")
        for i, doc in enumerate(docs):
            print(f"--- Chunk {i+1} ---")
            print("Contenido:", doc.page_content[:300], "..." if len(doc.page_content) > 300 else "")
            if hasattr(doc, "metadata"):
                print("Metadatos:", doc.metadata)
            print("-------------------")
        
        if not docs:
            return "‚ùå No encontr√© informaci√≥n relevante en los documentos de p√≥lizas."

        contenido = "\n\n".join([doc.page_content for doc in docs])
        if len(contenido.strip()) < 100:
            return "‚ùå No encontr√© informaci√≥n clara en los documentos."

        return f"‚úÖ Esto fue lo que encontr√© en los documentos de p√≥lizas:\n\n{contenido}"

    return Tool(
        name="ConsultaPDF",
        func=consulta,
        description="Consulta en la base de conocimiento de p√≥lizas"
    )

# Herramienta web
def crear_tool_web():
    return Tool(
        name="BusquedaEnLinea",
        func=DuckDuckGoSearchRun().run,
        description="Consulta en internet como √∫ltimo recurso"
    )

# Agente con Ollama - completamente gratuito
def responder(query: str, herramienta_pdf, herramienta_web, llm):
    system_prompt = (
        "Eres un agente de atenci√≥n al cliente especializado en p√≥lizas de seguros. "
        "Tu funci√≥n es responder preguntas relacionadas √∫nicamente con temas de seguros, p√≥lizas, coberturas, derechos del asegurado, etc. "
        "Si el usuario hace preguntas que no tienen que ver con seguros, debes responder con cortes√≠a indicando que no puedes ayudar con ese tema. "
        "Siempre debes responder con un tono amable, profesional, respetuoso y enfocado en ayudar al cliente. "
        "Responde en espa√±ol de manera clara y concisa."
    )
    
    # Primero buscar en PDFs locales
    respuesta_pdf = herramienta_pdf.run(query)
    
    if "No encontr√©" in respuesta_pdf:
        print("\n[DEBUG] No se encontr√≥ info en PDFs, buscando en web...")
        respuesta_web = herramienta_web.run(query)
        prompt = f"""{system_prompt}

Pregunta del cliente: {query}

Informaci√≥n encontrada en internet: {respuesta_web}

Por favor, responde bas√°ndote en esta informaci√≥n, pero solo si est√° relacionada con seguros o p√≥lizas. Si no est√° relacionada, indica cort√©smente que no puedes ayudar con ese tema."""
    else:
        prompt = f"""{system_prompt}

Pregunta del cliente: {query}

Informaci√≥n encontrada en nuestros documentos de p√≥lizas: {respuesta_pdf}

Por favor, responde bas√°ndote en esta informaci√≥n de nuestros documentos oficiales."""

    print("\n[DEBUG] Enviando prompt a Ollama...")
    try:
        response = llm.invoke(prompt)
        return response
    except Exception as e:
        return f"‚ùå Error al conectar con Ollama: {e}. ¬øEst√° corriendo el servicio?"

def main():
    print("ü¶ô Iniciando agente especializado en p√≥lizas con Ollama (GRATIS)...")
    
    # Verificar que Ollama est√© corriendo
    try:
        # Usar el modelo que tienes instalado
        llm = Ollama(
            model="llama3.2:latest",  # Puedes cambiar a "deepseek-r1:1.5b" si prefieres uno m√°s r√°pido
            temperature=0.1
        )
        
        # Test r√°pido
        print("üîÑ Probando conexi√≥n con Ollama...")
        test_response = llm.invoke("Hola")
        print("‚úÖ Conexi√≥n exitosa con Ollama")
        
    except Exception as e:
        print(f"‚ùå Error conectando con Ollama: {e}")
        print("üí° Aseg√∫rate de que Ollama est√© corriendo: 'ollama serve' en otra terminal")
        return

    # Cargar herramientas
    try:
        vectordb = cargar_vectordb()
        tool_pdf = crear_tool_pdf(vectordb)
        tool_web = crear_tool_web()
        print("‚úÖ Herramientas cargadas correctamente")
    except Exception as e:
        print(f"‚ùå Error cargando herramientas: {e}")
        return

    print("\nü§ñ Agente listo para consultas sobre seguros y p√≥lizas.")
    print("üí° Usando Ollama (completamente gratis) + embeddings de HuggingFace")
    print("Escribe 'salir' para terminar.\n")

    while True:
        pregunta = input("üßë T√∫: ")
        if pregunta.lower() in ["salir", "exit", "quit"]:
            break
        
        print("üîÑ Procesando...")
        respuesta = responder(pregunta, tool_pdf, tool_web, llm)
        print(f"\nü§ñ Agente:\n{respuesta}\n")

if __name__ == "__main__":
    main()