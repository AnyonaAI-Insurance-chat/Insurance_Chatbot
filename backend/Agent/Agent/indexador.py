# indexador.py - ChromaDB + Ollama (SIN OpenAI)

import os
import chromadb
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from dotenv import load_dotenv

load_dotenv()

def construir_y_guardar_vector_index():
    print("ğŸ“¥ Leyendo PDFs desde 'data/'...")
    archivos = [f for f in os.listdir("data") if f.endswith(".pdf")]
    if not archivos:
        print("âŒ No hay archivos PDF en la carpeta 'data'.")
        return

    print(f"ğŸ“š Encontrados {len(archivos)} PDFs:")
    for archivo in archivos:
        print(f"   - {archivo}")

    # Cliente ChromaDB persistente
    print("ğŸ—„ï¸ Inicializando ChromaDB...")
    client = chromadb.PersistentClient(path="./chroma_seguros")
    
    # Crear/obtener colecciÃ³n
    collection = client.get_or_create_collection(
        name="polizas_seguros",
        metadata={"description": "Documentos de pÃ³lizas de seguros"}
    )
    
    print("âœ… ColecciÃ³n ChromaDB creada/conectada")

    # Cargar PDFs
    print("ğŸ“– Cargando contenido de PDFs...")
    loaders = [PyPDFLoader(f"data/{f}") for f in archivos]
    documentos = []
    for i, loader in enumerate(loaders):
        print(f"   Procesando {archivos[i]}...")
        try:
            docs = loader.load()
            documentos.extend(docs)
            print(f"      âœ… {len(docs)} pÃ¡ginas extraÃ­das")
        except Exception as e:
            print(f"      âŒ Error: {e}")

    print(f"âœ… Total pÃ¡ginas cargadas: {len(documentos)}")

    # Dividir en chunks
    print("âœ‚ï¸ Dividiendo en chunks...")
    splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    chunks = splitter.split_documents(documentos)
    
    print(f"âœ… Creados {len(chunks)} chunks de texto")

    # Preparar datos para ChromaDB
    print("ğŸ”„ Preparando datos para ChromaDB...")
    texts = []
    metadatas = []
    ids = []
    
    for i, chunk in enumerate(chunks):
        texts.append(chunk.page_content)
        metadatas.append({
            "source": chunk.metadata.get("source", "unknown"),
            "page": chunk.metadata.get("page", 0),
            "chunk_id": i
        })
        ids.append(f"doc_{i}")

    # Agregar a ChromaDB (usa embeddings automÃ¡ticos)
    print("ğŸ’¾ Guardando en ChromaDB...")
    print("   (ChromaDB generarÃ¡ embeddings automÃ¡ticamente)")
    
    try:
        collection.add(
            documents=texts,
            metadatas=metadatas,
            ids=ids
        )
        
        print(f"âœ… Agregados {len(texts)} chunks a ChromaDB")
        print("âœ… Ãndice ChromaDB creado exitosamente en './chroma_seguros/'")
        print("ğŸš€ Ahora puedes ejecutar: python main.py")
        
    except Exception as e:
        print(f"âŒ Error guardando en ChromaDB: {e}")

if __name__ == "__main__":
    construir_y_guardar_vector_index()