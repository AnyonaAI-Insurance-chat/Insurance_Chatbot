import os
import asyncio
from typing import AsyncGenerator
import chromadb
from langchain_community.tools.ddg_search import DuckDuckGoSearchRun
from langchain.memory import ConversationBufferMemory
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain_core.prompts import ChatPromptTemplate
from langchain.callbacks.base import AsyncCallbackHandler
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.tools import Tool

# =========================================================================
# === INICIO: L√ìGICA ORIGINAL DEL EQUIPO DE IA (Adaptada para Docker) ===
# =========================================================================

def cargar_chromadb():
    """
    Se conecta al servicio de ChromaDB que corre en Docker.
    """
    try:
        # CORRECCI√ìN CLAVE: Apuntamos al servicio 'chromadb' de Docker
        client = chromadb.HttpClient(host='chromadb', port=8000)
        collection = client.get_collection("polizas_seguros")
        print("‚úÖ Base de datos ChromaDB cargada y conectada desde Docker.")
        return collection
    except Exception as e:
        # En un entorno de servidor, es mejor lanzar la excepci√≥n para que se registre.
        print(f"‚ùå Error fatal al cargar ChromaDB: {e}")
        raise e

def crear_tool_chromadb(collection):
    """
    Crea la herramienta de LangChain para consultar la base de datos de p√≥lizas.
    """
    def consulta_chromadb(query: str) -> str:
        try:
            results = collection.query(query_texts=[query], n_results=3)
            if not results['documents'][0]:
                return "No encontr√© informaci√≥n relevante en los documentos de p√≥lizas."
            
            contenido = "\n\n".join(results['documents'][0])
            if len(contenido.strip()) < 50:
                 return "No encontr√© informaci√≥n clara en los documentos de p√≥lizas."

            metadatos = ""
            if results['metadatas'] and results['metadatas'][0]:
                fuentes = {os.path.basename(m['source']) for m in results['metadatas'][0] if m and 'source' in m}
                if fuentes:
                    metadatos = f"\n\nFuentes: {', '.join(fuentes)}"
            return f"Informaci√≥n encontrada en documentos de p√≥lizas:\n\n{contenido}{metadatos}"
        except Exception as e:
            return f"Error consultando ChromaDB: {e}"
    
    return Tool(
        name="ConsultaPDF",
        func=consulta_chromadb,
        description="HERRAMIENTA OBLIGATORIA: Busca informaci√≥n en documentos PDF de p√≥lizas de seguros. DEBES usarla para TODA pregunta. Contiene 267 chunks de informaci√≥n sobre seguros, p√≥lizas, coberturas, etc."
    )

def crear_tool_buscador_noticias():
    """
    Crea una herramienta de b√∫squeda de noticias robusta que maneja queries vac√≠os.
    """
    # 1. Creamos una funci√≥n "envoltorio" (wrapper)
    def buscar_noticias_seguras(query: str) -> str:
        # 2. La validaci√≥n clave para evitar el crash
        if not query or not isinstance(query, str) or len(query.strip()) < 3:
            return "Para poder buscar noticias, necesito que me indiques un tema claro. Por ejemplo: 'noticias sobre seguros de ciberseguridad'."
        
        # 3. Si la validaci√≥n pasa, ejecutamos la b√∫squeda real
        print(f"üîé Buscando noticias en internet sobre: '{query}'")

        search_tool = DuckDuckGoSearchRun() 
        return search_tool.run(query)

    # 4. Creamos la herramienta con nuestra funci√≥n segura
    return Tool(
        name="buscar_noticias_del_sector",
        func=buscar_noticias_seguras,
        description="√ösala cuando el usuario pida 'noticias', 'actualidad' o 'novedades' sobre un TEMA ESPEC√çFICO de seguros. Esta herramienta requiere un t√©rmino de b√∫squeda claro."
    )
def crear_tool_recombinador(collection, llm):
    def recombinar_coberturas(temas_a_combinar: str) -> str:
        """
        Busca varios textos de cobertura sobre los temas dados y los fusiona en uno nuevo.
        Ejemplo de input: 'cobertura de hospitalizaci√≥n y cobertura ambulatoria'
        """
        print(f"ü§ñ Recombinando coberturas para los temas: {temas_a_combinar}")
        try:
            # 1. Buscar fragmentos de texto para cada tema
            temas = [tema.strip() for tema in temas_a_combinar.split("y")]
            textos_encontrados = []
            for tema in temas:
                query = f"Art√≠culo 2 sobre cobertura de {tema}"
                results = collection.query(query_texts=[query], n_results=2)
                if results and results.get('documents') and results['documents'][0]:
                    textos_encontrados.extend(results['documents'][0])
            
            if not textos_encontrados:
                return "No encontr√© suficientes textos de cobertura sobre los temas que mencionaste para poder crear una nueva p√≥liza."

            contexto_unificado = "\n\n== FIN DE UN DOCUMENTO ==\n\n".join(textos_encontrados)

            # 2. Usar el LLM con un prompt espec√≠fico para la tarea de fusi√≥n
            prompt_fusion = f"""Eres un abogado experto en seguros. Tu tarea es redactar un nuevo y √∫nico 'ART√çCULO 2: COBERTURA' para una p√≥liza de salud.
            Debes basarte EXCLUSIVAMENTE en los siguientes fragmentos de p√≥lizas existentes. Combina las ideas, elimina redundancias y crea un texto coherente, claro y completo.
            El resultado debe ser un solo art√≠culo unificado.

            FRAGMENTOS DE P√ìLIZAS EXISTENTES:
            ---
            {contexto_unificado}
            ---

            NUEVO ART√çCULO 2: COBERTURA (Redacci√≥n unificada):
            """
            
            # 3. Invocar al LLM para la tarea de generaci√≥n
            response = llm.invoke(prompt_fusion)
            return response.content

        except Exception as e:
            return f"Ocurri√≥ un error durante el proceso de recombinaci√≥n: {e}"

    return Tool(
        name="crear_nueva_cobertura_combinada",
        func=recombinar_coberturas,
        description="Herramienta avanzada. √ösala SOLO cuando el usuario pida expl√≠citamente 'crear una nueva p√≥liza', 'combinar coberturas', 'fusionar art√≠culos' o 'crear un ejemplo de cobertura' a partir de temas existentes."
    )

def crear_agente_executor(tools, llm):
    """
    Crea el agente y su ejecutor usando el m√©todo moderno y robusto `create_tool_calling_agent`.
    """
    # 1. El Prompt. Es m√°s estructurado.
    #    Aqu√≠ definimos el personaje y las reglas de forma inamovible.
    prompt = ChatPromptTemplate.from_messages([
        ("system", """Eres "Asistente Protege+", un especialista amable, emp√°tico y muy competente en p√≥lizas de seguros. Tu objetivo es hacer que el usuario se sienta acompa√±ado y comprendido.
        == TUS HERRAMIENTAS Y CU√ÅNDO USARLAS ==
        1.  `consultar_polizas_seguros`: Tu herramienta por defecto. Si la pregunta es sobre coberturas, condiciones, art√≠culos o cualquier detalle de una p√≥liza, esta es tu elecci√≥n.
        2.  `buscar_noticias_del_sector`: Si el usuario menciona 'noticias', 'actualidad' o 'novedades' sobre un tema, usa esta herramienta. Aseg√∫rate de extraer el tema de la pregunta del usuario para usarlo como t√©rmino de b√∫squeda.
        3.  `crear_nueva_cobertura_combinada`: Misi√≥n especial. √ösala SOLO si el usuario pide expl√≠citamente 'crear', 'combinar', 'fusionar' o 'hacer un ejemplo' de una nueva cobertura a partir de otras existentes.

        == TU PERSONALIDAD Y FORMA DE HABLAR ==
        - **Tono:** Eres profesional pero cercano. Usa un lenguaje claro y evita la jerga compleja.
        - **Proactividad:** Gu√≠a al usuario. Si una pregunta es ambigua, pide aclaraci√≥n.
        - **Transparencia en la acci√≥n:** Comunica lo que est√°s haciendo. Usa frases como "Claro, d√©jame revisar los detalles en tu p√≥liza.", "Entendido, consultando la informaci√≥n...".

        == REGLAS DE ORO (INQUEBRANTABLES) ==
        1.  **IDENTIDAD:** Eres "Asistente Protege+". Nunca menciones que eres una IA, un modelo o de Google. Si te preguntan sobre tu naturaleza, responde con amabilidad: "Soy Asistente Protege+, tu especialista en p√≥lizas. Mi prop√≥sito es ayudarte a navegar tus documentos de seguro. ¬øEn qu√© te puedo ayudar?".
        2.  **FOCO:** Tu mundo son los seguros. Si la pregunta es de otro tema, rech√°zala cort√©smente.
        3.  **USO DE HERRAMIENTAS:** Para cualquier pregunta sobre seguros, tu acci√≥n principal es usar la herramienta `consultar_polizas_seguros`.

        == ESTRUCTURA DE RESPUESTA (Cuando encuentras informaci√≥n) ==
        1.  Confirma la recepci√≥n de la pregunta.
        2.  Presenta la informaci√≥n de forma clara, resumiendo y usando vi√±etas.
        3.  Cierra con una pregunta abierta para invitar a seguir la conversaci√≥n.

        # ================================================================= #
        # === REGLA CR√çTICA: MANEJO DE CONVERSACI√ìN Y MEMORIA CONTEXTUAL ==== #
        # ================================================================= #
        PRESTA MUCHA ATENCI√ìN al historial del chat (`chat_history`). Las preguntas cortas del usuario como 'y por qu√©?', 'd√≥nde dice eso?', 'pero de que articulo es?' o 'y sobre la cobertura X?' NO tienen sentido por s√≠ solas.
        ANTES DE RESPONDER, DEBES leer los mensajes anteriores para entender el contexto completo.

        EJEMPLO DE RAZONAMIENTO CORRECTO:
        - Historial: El usuario acaba de enviar el texto "Los reembolsos se efectuar√°n...".
        - Pregunta Actual: "pero de que articulo es?"
        - TU PENSAMIENTO: 'La pregunta 'de qu√© art√≠culo es' se refiere al texto sobre 'reembolsos' que el usuario me dio justo antes. Mi tarea es tomar ese texto y buscarlo con mi herramienta `consultar_polizas_seguros` para encontrar su ubicaci√≥n en la p√≥liza.'
        
        Si el usuario te da un texto de la p√≥liza y luego te pregunta d√≥nde est√°, TU DEBER es usar la herramienta para buscar ESE texto.
        """),
        ("placeholder", "{chat_history}"),
        ("human", "{input}"),
        ("placeholder", "{agent_scratchpad}"),
    ])

    # 2. Creamos el agente. Este agente est√° dise√±ado para llamar a herramientas.
    agent = create_tool_calling_agent(llm, tools, prompt)

    # 3. Creamos el ejecutor del agente, que maneja el ciclo de ejecuci√≥n.
    #    La memoria se pasa aqu√≠.
    memory = ConversationBufferMemory(
        memory_key="chat_history", 
        return_messages=True, 
        output_key="output"
    )
    
    return AgentExecutor(
        agent=agent, 
        tools=tools, 
        verbose=True, 
        memory=memory,
        handle_parsing_errors=True # Muy importante para producci√≥n
    )

# --- CONFIGURACI√ìN GLOBAL  ---



collection = cargar_chromadb()




# 2. Crear las herramientas
tool_chromadb = crear_tool_chromadb(collection)
tool_web = crear_tool_web()
tools = [tool_chromadb, tool_web]

# 3. Configurar el LLM
# Usamos una variable de entorno para la URL de Ollama, con un valor por defecto
# para que Docker pueda comunicarse con el Ollama que corre en el host (tu PC).

google_api_key = os.getenv("GOOGLE_API_KEY") 
if not google_api_key:
    raise ValueError("La variable de entorno GOOGLE_API_KEY no est√° configurada.")

llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash-latest", # O el modelo de Gemini que prefieras
    google_api_key=google_api_key,

    temperature=0.0,

    temperature=0.1,
    convert_system_message_to_human=True # √ötil para compatibilidad con algunos agentes

)


print("‚úÖ L√≥gica del agente y herramientas inicializadas.")


tools = [
    crear_tool_chromadb(collection),
    crear_tool_buscador_noticias(),
    crear_tool_recombinador(collection, llm) 
]

# =========================================================================
# === FIN: L√ìGICA ORIGINAL --- INICIO: CAPA DE CONEXI√ìN WEB (Streaming) ===
# =========================================================================


async def get_agent_response(question: str, session_id: str) -> AsyncGenerator[str, None]:
    """

    Funci√≥n puente que ahora usa el `AgentExecutor` con el agente `Tool Calling`.
    """
    # Creamos una instancia fresca del ejecutor para cada petici√≥n para mantener el estado aislado.
    agent_executor = crear_agente_executor(tools, llm)

    async for chunk in agent_executor.astream({"input": question}):
        # La salida de astream puede variar, buscamos la respuesta final en 'output'
        if "output" in chunk:
            cleaned_output = chunk["output"].replace("```", "").strip()
            if cleaned_output:
                yield cleaned_output

    Funci√≥n "puente" que usa las funciones del equipo de IA para responder
    a una petici√≥n web de forma as√≠ncrona y con streaming, usando el m√©todo nativo
    de LangChain `.astream()`.
    """
    # Creamos una instancia fresca del agente para cada petici√≥n.
    # Ya no necesitamos pasarle el callback.
    agent_executor = crear_agente(tools, llm)

    # El m√©todo .astream() es un generador as√≠ncrono que devuelve
    # los pasos del pensamiento del agente en tiempo real.
    async for chunk in agent_executor.astream({"input": question}):
        # El agente devuelve diferentes tipos de "chunks" (pasos de acci√≥n, observaciones, etc.)
        # A nosotros solo nos interesa el chunk final que contiene la respuesta del LLM.
        # Este chunk se identifica porque tiene la clave "output".
        if "output" in chunk:
            # Hacemos yield de cada trozo de la respuesta final a medida que llega.
            yield chunk["output"]

