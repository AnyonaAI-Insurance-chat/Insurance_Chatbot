# indexador.py - ChromaDB + Ollama (SIN OpenAI)

import os
import chromadb
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from dotenv import load_dotenv
# --- NUEVAS L√çNEAS ---
# Esto encuentra la ruta del directorio donde est√° el script (ej: /code/scripts)
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
# Esto sube un nivel para llegar a la ra√≠z del proyecto (ej: /code)
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)
# Definimos la ruta a la carpeta de datos
DATA_DIR = os.path.join(PROJECT_ROOT, "data")
load_dotenv()

def construir_y_guardar_vector_index():
    print("üì• Leyendo PDFs desde 'data/'...")
    archivos = [f for f in os.listdir(DATA_DIR) if f.endswith(".pdf")]
    if not archivos:
        print("‚ùå No hay archivos PDF en la carpeta 'data'.")
        return

    print(f"üìö Encontrados {len(archivos)} PDFs:")
    for archivo in archivos:
        print(f"   - {archivo}")

    # Cliente ChromaDB persistente
    print("üóÑÔ∏è Inicializando ChromaDB...")
   
    client = chromadb.HttpClient(host='chromadb', port=8000)
        # Hacemos un "ping" para asegurarnos de que la conexi√≥n funciona
    client.heartbeat() 
    print("‚úÖ Conexi√≥n con ChromaDB establecida.")

    
    # Crear/obtener colecci√≥n
    collection = client.get_or_create_collection(
        name="polizas_seguros",
        metadata={"description": "Documentos de p√≥lizas de seguros"}
    )
    
    print("‚úÖ Colecci√≥n ChromaDB creada/conectada")

    # Cargar PDFs
    print("üìñ Cargando contenido de PDFs...")
    loaders = [PyPDFLoader(os.path.join(DATA_DIR, f)) for f in archivos]
    documentos = []
    for i, loader in enumerate(loaders):
        print(f"   Procesando {archivos[i]}...")
        try:
            docs = loader.load()
            documentos.extend(docs)
            print(f"      ‚úÖ {len(docs)} p√°ginas extra√≠das")
        except Exception as e:
            print(f"      ‚ùå Error: {e}")

    print(f"‚úÖ Total p√°ginas cargadas: {len(documentos)}")

    # Dividir en chunks
    print("‚úÇÔ∏è Dividiendo en chunks...")
    splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    chunks = splitter.split_documents(documentos)
    
    print(f"‚úÖ Creados {len(chunks)} chunks de texto")

    # Preparar datos para ChromaDB
    print("üîÑ Preparando datos para ChromaDB...")
    texts = []
    metadatas = []
    ids = []
    
    for i, chunk in enumerate(chunks):
        texts.append(chunk.page_content)
        metadatas.append({
            "source": chunk.metadata.get("source", "unknown"),
            "page": chunk.metadata.get("page", 0),
            "chunk_id": i
        })
        ids.append(f"doc_{i}")

    # Agregar a ChromaDB (usa embeddings autom√°ticos)
    print("üíæ Guardando en ChromaDB...")
    print("   (ChromaDB generar√° embeddings autom√°ticamente)")
    
    try:
        collection.add(
            documents=texts,
            metadatas=metadatas,
            ids=ids
        )
        
        print(f"‚úÖ Agregados {len(texts)} chunks a ChromaDB")
        print("‚úÖ √çndice ChromaDB creado exitosamente en './chroma_seguros/'")
        print("üöÄ Ahora puedes ejecutar: python main.py")
        
    except Exception as e:
        print(f"‚ùå Error guardando en ChromaDB: {e}")

if __name__ == "__main__":
    construir_y_guardar_vector_index()
